@article{Guo2023c,
 abstract = {<div data-language="eng" data-ev-field="abstract">In this study, we present a novel algorithm, based on synchronous policy iteration, to solve the continuous-time infinite-horizon optimal control problem of input affine system dynamics. The integral reinforcement is measured as an excitation signal to estimate the solution to the Hamilton&ndash;Jacobi&ndash;Bellman equation. In addition, the proposed method is completely model-free, that is, no a priori knowledge of the system is required. Using the adaptive tuning law, the actor and critic neural networks can simultaneously approximate the optimal value function and policy. The persistence of excitation condition is required to guarantee the convergence of the two networks. Unlike in traditional policy iteration algorithms, the restriction of the initial admissible policy was eliminated using this method. The effectiveness of the proposed algorithm is verified through numerical simulations.<br/></div> &copy; 2022 Elsevier B.V.},
 author = {Guo, Lei and Zhao, Han},
 copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
 doi = {10.1016/j.neucom.2022.11.055},
 earlyaccessdate = {DEC 2022},
 eissn = {1872-8286},
 issn = {0925-2312},
 journal = {NEUROCOMPUTING},
 key = {Reinforcement learning},
 keywords = {Adaptive control systems;Continuous time systems;Dynamic programming;E-learning;Iterative methods;Learning algorithms;Optimal control systems;},
 language = {English},
 month = {FEB 1},
 note = {Actor critic;Adaptive Control;Adaptive optimal control;Continous time;Infinite horizons;Neural-networks;Novel algorithm;Optimal control algorithm;Policy iteration;Reinforcement learnings;},
 pages = {250-261},
 title = {Online adaptive optimal control algorithm based on synchronous integral reinforcement learning with explorations},
 unique-id = {WOS:000904659700002},
 url = {http://dx.doi.org/10.1016/j.neucom.2022.11.055},
 volume = {520},
 year = {2023}
}
